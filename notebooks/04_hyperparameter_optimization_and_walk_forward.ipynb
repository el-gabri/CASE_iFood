{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparametrização e Validação Walk-Forward para Detecção de Fraude\n",
    "\n",
    "Este notebook implementa a otimização de hiperparâmetros para cada modelo e realiza a validação walk-forward para avaliação temporal robusta do sistema de detecção de fraude.\n",
    "\n",
    "## Conteúdo\n",
    "1. [Configuração do Ambiente](#1.-Configuração-do-Ambiente)\n",
    "2. [Carregamento dos Dados](#2.-Carregamento-dos-Dados)\n",
    "3. [Otimização de Hiperparâmetros](#3.-Otimização-de-Hiperparâmetros)\n",
    "   - [Regressão Logística](#3.1-Regressão-Logística)\n",
    "   - [Random Forest](#3.2-Random-Forest)\n",
    "   - [Gradient Boosted Trees](#3.3-Gradient-Boosted-Trees)\n",
    "   - [XGBoost](#3.4-XGBoost)\n",
    "4. [Validação Walk-Forward](#4.-Validação-Walk-Forward)\n",
    "   - [Configuração da Validação](#4.1-Configuração-da-Validação)\n",
    "   - [Execução da Validação](#4.2-Execução-da-Validação)\n",
    "   - [Análise dos Resultados](#4.3-Análise-dos-Resultados)\n",
    "5. [Comparação de Modelos](#5.-Comparação-de-Modelos)\n",
    "6. [Modelo Final](#6.-Modelo-Final)\n",
    "7. [Conclusões](#7.-Conclusões)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Vamos importar as bibliotecas necessárias e configurar o ambiente para nossa análise."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T00:06:49.325962Z",
     "start_time": "2025-05-22T00:06:48.439734Z"
    }
   },
   "source": [
    "# Importar bibliotecas necessárias\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Bibliotecas de machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configurações de visualização\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configurar exibição de dados no pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos para os arquivos\n",
    "# Ajuste os caminhos conforme necessário para seu ambiente\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "reports_dir = os.path.join(base_dir, 'reports')\n",
    "\n",
    "# Garantir que os diretórios existem\n",
    "for directory in [processed_dir, models_dir, reports_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados\n",
    "\n",
    "Vamos carregar os dados processados que foram preparados nos notebooks anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos para os arquivos de dados processados\n",
    "train_path = os.path.join(processed_dir, 'train_transformed.csv')\n",
    "test_path = os.path.join(processed_dir, 'test_transformed.csv')\n",
    "feature_cols_path = os.path.join(processed_dir, 'feature_cols.txt')\n",
    "\n",
    "# Verificar se os arquivos existem\n",
    "if not all(os.path.exists(p) for p in [train_path, test_path]):\n",
    "    print(f\"Erro: Arquivos de dados processados não encontrados em {processed_dir}\")\n",
    "    print(\"Execute os notebooks de preparação de dados e feature engineering primeiro.\")\n",
    "else:\n",
    "    # Carregar os dados\n",
    "    print(\"Carregando dados processados...\")\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Carregar lista de features se disponível\n",
    "    if os.path.exists(feature_cols_path):\n",
    "        with open(feature_cols_path, 'r') as f:\n",
    "            feature_cols = [line.strip() for line in f.readlines()]\n",
    "    else:\n",
    "        # Identificar colunas de features (todas exceto a target)\n",
    "        feature_cols = [col for col in train_df.columns if col != 'is_fraud']\n",
    "    \n",
    "    # Extrair features e target\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['is_fraud']\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df['is_fraud']\n",
    "    \n",
    "    print(f\"Conjunto de treino: {X_train.shape[0]} amostras, {X_train.shape[1]} features\")\n",
    "    print(f\"Conjunto de teste: {X_test.shape[0]} amostras, {X_test.shape[1]} features\")\n",
    "    print(f\"Distribuição da classe alvo (treino): {np.bincount(y_train)}\")\n",
    "    print(f\"Distribuição da classe alvo (teste): {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar as primeiras linhas dos dados\n",
    "print(\"Primeiras linhas do conjunto de treino:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Otimização de Hiperparâmetros\n",
    "\n",
    "Vamos otimizar os hiperparâmetros de cada modelo para melhorar seu desempenho. Usaremos validação cruzada temporal (TimeSeriesSplit) para garantir que a avaliação respeite a ordem cronológica dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar validação cruzada temporal\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Visualizar os splits temporais\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X_train)):\n",
    "    print(f\"Split {i+1}:\")\n",
    "    print(f\"  Treino: índices {train_index[0]} até {train_index[-1]} ({len(train_index)} amostras)\")\n",
    "    print(f\"  Teste: índices {test_index[0]} até {test_index[-1]} ({len(test_index)} amostras)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Regressão Logística\n",
    "\n",
    "Vamos otimizar os hiperparâmetros do modelo de Regressão Logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_logistic_regression(X_train, y_train, cv=3, n_jobs=-1):\n",
    "    \"\"\"Otimiza hiperparâmetros para o modelo de Regressão Logística.\"\"\"\n",
    "    print(\"Otimizando hiperparâmetros para Regressão Logística...\")\n",
    "    \n",
    "    # Definir modelo base\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Definir grade de hiperparâmetros\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    \n",
    "    # Realizar busca em grade\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo com diferentes combinações de hiperparâmetros\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter melhor modelo e hiperparâmetros\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Melhores hiperparâmetros: {best_params}\")\n",
    "    print(f\"Melhor F1-Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return best_model, best_params, grid_search.cv_results_\n",
    "\n",
    "# Otimizar hiperparâmetros para Regressão Logística\n",
    "# Nota: Esta célula pode levar algum tempo para executar\n",
    "lr_model, lr_params, lr_results = optimize_logistic_regression(X_train, y_train, cv=tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados da otimização\n",
    "lr_results_df = pd.DataFrame(lr_results)\n",
    "lr_results_df = lr_results_df.sort_values('rank_test_score')\n",
    "lr_results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest\n",
    "\n",
    "Vamos otimizar os hiperparâmetros do modelo Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest(X_train, y_train, cv=3, n_jobs=-1):\n",
    "    \"\"\"Otimiza hiperparâmetros para o modelo Random Forest.\"\"\"\n",
    "    print(\"Otimizando hiperparâmetros para Random Forest...\")\n",
    "    \n",
    "    # Definir modelo base\n",
    "    model = RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    # Definir grade de hiperparâmetros\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    \n",
    "    # Usar RandomizedSearchCV para otimização (mais eficiente para muitos hiperparâmetros)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Número de combinações a testar\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo com diferentes combinações de hiperparâmetros\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter melhor modelo e hiperparâmetros\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    \n",
    "    print(f\"Melhores hiperparâmetros: {best_params}\")\n",
    "    print(f\"Melhor F1-Score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return best_model, best_params, random_search.cv_results_\n",
    "\n",
    "# Otimizar hiperparâmetros para Random Forest\n",
    "# Nota: Esta célula pode levar algum tempo para executar\n",
    "rf_model, rf_params, rf_results = optimize_random_forest(X_train, y_train, cv=tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados da otimização\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "rf_results_df = rf_results_df.sort_values('rank_test_score')\n",
    "rf_results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Gradient Boosted Trees\n",
    "\n",
    "Vamos otimizar os hiperparâmetros do modelo Gradient Boosted Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_gradient_boosting(X_train, y_train, cv=3, n_jobs=-1):\n",
    "    \"\"\"Otimiza hiperparâmetros para o modelo Gradient Boosted Trees.\"\"\"\n",
    "    print(\"Otimizando hiperparâmetros para Gradient Boosted Trees...\")\n",
    "    \n",
    "    # Definir modelo base\n",
    "    model = GradientBoostingClassifier(\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Definir grade de hiperparâmetros\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Usar RandomizedSearchCV para otimização (mais eficiente para muitos hiperparâmetros)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Número de combinações a testar\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo com diferentes combinações de hiperparâmetros\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter melhor modelo e hiperparâmetros\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    \n",
    "    print(f\"Melhores hiperparâmetros: {best_params}\")\n",
    "    print(f\"Melhor F1-Score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return best_model, best_params, random_search.cv_results_\n",
    "\n",
    "# Otimizar hiperparâmetros para Gradient Boosted Trees\n",
    "# Nota: Esta célula pode levar algum tempo para executar\n",
    "gb_model, gb_params, gb_results = optimize_gradient_boosting(X_train, y_train, cv=tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados da otimização\n",
    "gb_results_df = pd.DataFrame(gb_results)\n",
    "gb_results_df = gb_results_df.sort_values('rank_test_score')\n",
    "gb_results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost\n",
    "\n",
    "Vamos otimizar os hiperparâmetros do modelo XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_xgboost(X_train, y_train, cv=3, n_jobs=-1):\n",
    "    \"\"\"Otimiza hiperparâmetros para o modelo XGBoost.\"\"\"\n",
    "    print(\"Otimizando hiperparâmetros para XGBoost...\")\n",
    "    \n",
    "    # Calcular scale_pos_weight para lidar com desbalanceamento\n",
    "    scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    \n",
    "    # Definir modelo base\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    # Definir grade de hiperparâmetros\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Usar RandomizedSearchCV para otimização (mais eficiente para muitos hiperparâmetros)\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Número de combinações a testar\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo com diferentes combinações de hiperparâmetros\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter melhor modelo e hiperparâmetros\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    \n",
    "    print(f\"Melhores hiperparâmetros: {best_params}\")\n",
    "    print(f\"Melhor F1-Score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return best_model, best_params, random_search.cv_results_\n",
    "\n",
    "# Otimizar hiperparâmetros para XGBoost\n",
    "# Nota: Esta célula pode levar algum tempo para executar\n",
    "xgb_model, xgb_params, xgb_results = optimize_xgboost(X_train, y_train, cv=tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados da otimização\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "xgb_results_df = xgb_results_df.sort_values('rank_test_score')\n",
    "xgb_results_df[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenar modelos otimizados e seus hiperparâmetros\n",
    "optimized_models = {\n",
    "    'Logistic Regression': (lr_model, lr_params),\n",
    "    'Random Forest': (rf_model, rf_params),\n",
    "    'Gradient Boosted Trees': (gb_model, gb_params),\n",
    "    'XGBoost': (xgb_model, xgb_params)\n",
    "}\n",
    "\n",
    "# Salvar resultados da otimização de hiperparâmetros\n",
    "with open(os.path.join(reports_dir, 'hyperparameter_optimization_results.txt'), 'w') as f:\n",
    "    f.write(\"RESULTADOS DA OTIMIZAÇÃO DE HIPERPARÂMETROS\\n\")\n",
    "    f.write(\"=========================================\\n\\n\")\n",
    "    \n",
    "    for model_name, (model, params) in optimized_models.items():\n",
    "        f.write(f\"{model_name}:\\n\")\n",
    "        f.write(\"-\" * len(model_name) + \"\\n\")\n",
    "        \n",
    "        for param, value in params.items():\n",
    "            f.write(f\"  {param}: {value}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Salvar modelos otimizados\n",
    "for model_name, (model, _) in optimized_models.items():\n",
    "    joblib.dump(model, os.path.join(models_dir, f\"{model_name.replace(' ', '_').lower()}_optimized.joblib\"))\n",
    "\n",
    "print(f\"Resultados da otimização de hiperparâmetros salvos em: {reports_dir}\")\n",
    "print(f\"Modelos otimizados salvos em: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validação Walk-Forward\n",
    "\n",
    "A validação walk-forward é uma técnica de validação temporal que respeita a ordem cronológica dos dados. Ela é especialmente importante para problemas de detecção de fraude, onde os padrões podem mudar ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Configuração da Validação\n",
    "\n",
    "Vamos configurar a validação walk-forward, definindo o número de splits e o tamanho do conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation(df, feature_cols, target_col='is_fraud', n_splits=5, test_size=0.2):\n",
    "    \"\"\"Realiza validação walk-forward para avaliação temporal robusta.\"\"\"\n",
    "    print(\"Realizando validação walk-forward...\")\n",
    "    \n",
    "    # Garantir que o DataFrame está ordenado por tempo\n",
    "    if 'transaction_timestamp' in df.columns:\n",
    "        df = df.sort_values('transaction_timestamp')\n",
    "    elif 'unix_time' in df.columns:\n",
    "        df = df.sort_values('unix_time')\n",
    "    \n",
    "    # Extrair features e target\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Criar splits temporais\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=int(len(df) * test_size))\n",
    "    \n",
    "    # Inicializar modelos com os hiperparâmetros otimizados\n",
    "    models = {\n",
    "        'Logistic Regression': lr_model,\n",
    "        'Random Forest': rf_model,\n",
    "        'Gradient Boosted Trees': gb_model,\n",
    "        'XGBoost': xgb_model\n",
    "    }\n",
    "    \n",
    "    # Inicializar dicionário para armazenar resultados\n",
    "    results = {model_name: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []} \n",
    "               for model_name in models.keys()}\n",
    "    \n",
    "    # Realizar validação walk-forward\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\nSplit {i+1}/{n_splits}\")\n",
    "        \n",
    "        # Dividir dados em treino e teste\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Treinar e avaliar cada modelo\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"Treinando {model_name}...\")\n",
    "            \n",
    "            # Treinar modelo\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Fazer previsões\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calcular métricas\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_prob)\n",
    "            \n",
    "            # Armazenar resultados\n",
    "            results[model_name]['accuracy'].append(accuracy)\n",
    "            results[model_name]['precision'].append(precision)\n",
    "            results[model_name]['recall'].append(recall)\n",
    "            results[model_name]['f1'].append(f1)\n",
    "            results[model_name]['auc'].append(auc)\n",
    "            \n",
    "            # Imprimir métricas\n",
    "            print(f\"  Acurácia: {accuracy:.4f}\")\n",
    "            print(f\"  Precisão: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1-Score: {f1:.4f}\")\n",
    "            print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    # Calcular médias das métricas\n",
    "    for model_name in results.keys():\n",
    "        for metric in results[model_name].keys():\n",
    "            results[model_name][f'mean_{metric}'] = np.mean(results[model_name][metric])\n",
    "    \n",
    "    # Imprimir resultados finais\n",
    "    print(\"\\nResultados finais (média das métricas):\")\n",
    "    for model_name in results.keys():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Acurácia: {results[model_name]['mean_accuracy']:.4f}\")\n",
    "        print(f\"  Precisão: {results[model_name]['mean_precision']:.4f}\")\n",
    "        print(f\"  Recall: {results[model_name]['mean_recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {results[model_name]['mean_f1']:.4f}\")\n",
    "        print(f\"  AUC-ROC: {results[model_name]['mean_auc']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Execução da Validação\n",
    "\n",
    "Vamos executar a validação walk-forward com os modelos otimizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar validação walk-forward\n",
    "# Nota: Esta célula pode levar algum tempo para executar\n",
    "wf_results = walk_forward_validation(train_df, feature_cols, n_splits=5, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Análise dos Resultados\n",
    "\n",
    "Vamos analisar os resultados da validação walk-forward, visualizando como o desempenho dos modelos evolui ao longo do tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_walk_forward_results(results, output_dir=None):\n",
    "    \"\"\"Plota os resultados da validação walk-forward.\"\"\"\n",
    "    # Criar diretório para salvar os gráficos\n",
    "    if output_dir is not None:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Métricas para plotar\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    \n",
    "    # Plotar cada métrica\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plotar resultados para cada modelo\n",
    "        for model_name in results.keys():\n",
    "            plt.plot(range(1, len(results[model_name][metric]) + 1), \n",
    "                     results[model_name][metric], \n",
    "                     marker='o', \n",
    "                     label=f\"{model_name} (média: {results[model_name][f'mean_{metric}']:.4f})\")\n",
    "        \n",
    "        # Configurar gráfico\n",
    "        plt.xlabel('Split', fontsize=12)\n",
    "        plt.ylabel(metric.capitalize(), fontsize=12)\n",
    "        plt.title(f'Resultados da Validação Walk-Forward - {metric.capitalize()}', fontsize=14)\n",
    "        plt.xticks(range(1, len(results[list(results.keys())[0]][metric]) + 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Salvar gráfico\n",
    "        if output_dir is not None:\n",
    "            plt.savefig(os.path.join(output_dir, f'walk_forward_{metric}.png'), \n",
    "                        dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Plotar comparação das médias das métricas\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Configurar largura das barras\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(results))\n",
    "    \n",
    "    # Plotar barras para cada métrica\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[model_name][f'mean_{metric}'] for model_name in results.keys()]\n",
    "        plt.bar(index + i*bar_width, values, bar_width, label=metric.upper())\n",
    "    \n",
    "    # Configurar eixos e legendas\n",
    "    plt.xlabel('Modelo', fontsize=12)\n",
    "    plt.ylabel('Valor', fontsize=12)\n",
    "    plt.title('Comparação de Métricas por Modelo (Validação Walk-Forward)', fontsize=14)\n",
    "    plt.xticks(index + bar_width * (len(metrics) - 1) / 2, results.keys())\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[model_name][f'mean_{metric}'] for model_name in results.keys()]\n",
    "        for j, value in enumerate(values):\n",
    "            plt.text(j + i*bar_width, value + 0.01, f'{value:.3f}', \n",
    "                    ha='center', va='bottom', rotation=90, fontsize=10)\n",
    "    \n",
    "    # Salvar gráfico\n",
    "    if output_dir is not None:\n",
    "        plt.savefig(os.path.join(output_dir, 'walk_forward_comparison.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotar resultados da validação walk-forward\n",
    "plot_walk_forward_results(wf_results, reports_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados da validação walk-forward\n",
    "with open(os.path.join(reports_dir, 'walk_forward_results.txt'), 'w') as f:\n",
    "    f.write(\"RESULTADOS DA VALIDAÇÃO WALK-FORWARD\\n\")\n",
    "    f.write(\"==================================\\n\\n\")\n",
    "    \n",
    "    for model_name in wf_results.keys():\n",
    "        f.write(f\"{model_name}:\\n\")\n",
    "        f.write(\"-\" * len(model_name) + \"\\n\")\n",
    "        \n",
    "        f.write(f\"  Acurácia: {wf_results[model_name]['mean_accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Precisão: {wf_results[model_name]['mean_precision']:.4f}\\n\")\n",
    "        f.write(f\"  Recall: {wf_results[model_name]['mean_recall']:.4f}\\n\")\n",
    "        f.write(f\"  F1-Score: {wf_results[model_name]['mean_f1']:.4f}\\n\")\n",
    "        f.write(f\"  AUC-ROC: {wf_results[model_name]['mean_auc']:.4f}\\n\\n\")\n",
    "\n",
    "print(f\"Resultados da validação walk-forward salvos em: {os.path.join(reports_dir, 'walk_forward_results.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparação de Modelos\n",
    "\n",
    "Vamos comparar o desempenho dos modelos otimizados usando a validação walk-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados médios\n",
    "mean_results = []\n",
    "for model_name in wf_results.keys():\n",
    "    mean_results.append({\n",
    "        'model': model_name,\n",
    "        'accuracy': wf_results[model_name]['mean_accuracy'],\n",
    "        'precision': wf_results[model_name]['mean_precision'],\n",
    "        'recall': wf_results[model_name]['mean_recall'],\n",
    "        'f1': wf_results[model_name]['mean_f1'],\n",
    "        'auc': wf_results[model_name]['mean_auc']\n",
    "    })\n",
    "\n",
    "mean_results_df = pd.DataFrame(mean_results)\n",
    "mean_results_df = mean_results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"Comparação dos modelos (ordenados por F1-Score):\")\n",
    "mean_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelo Final\n",
    "\n",
    "Vamos identificar o melhor modelo com base no F1-Score médio da validação walk-forward e salvá-lo como o modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar melhor modelo com base no F1-Score médio\n",
    "best_model_name = max(wf_results.keys(), key=lambda k: wf_results[k]['mean_f1'])\n",
    "best_model = optimized_models[best_model_name][0]\n",
    "\n",
    "print(f\"Melhor modelo baseado na validação walk-forward: {best_model_name}\")\n",
    "print(f\"F1-Score médio: {wf_results[best_model_name]['mean_f1']:.4f}\")\n",
    "print(f\"AUC-ROC médio: {wf_results[best_model_name]['mean_auc']:.4f}\")\n",
    "\n",
    "# Salvar melhor modelo\n",
    "best_model_path = os.path.join(models_dir, 'best_model_walk_forward.joblib')\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"Melhor modelo salvo em: {best_model_path}\")\n",
    "\n",
    "# Criar resumo do melhor modelo\n",
    "summary_path = os.path.join(reports_dir, 'best_model_walk_forward_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"RESUMO DO MELHOR MODELO (VALIDAÇÃO WALK-FORWARD)\\n\")\n",
    "    f.write(\"=============================================\\n\\n\")\n",
    "    f.write(f\"Melhor modelo: {best_model_name}\\n\")\n",
    "    f.write(f\"AUC-ROC médio: {wf_results[best_model_name]['mean_auc']:.4f}\\n\")\n",
    "    f.write(f\"Precisão média (classe fraude): {wf_results[best_model_name]['mean_precision']:.4f}\\n\")\n",
    "    f.write(f\"Recall médio (classe fraude): {wf_results[best_model_name]['mean_recall']:.4f}\\n\")\n",
    "    f.write(f\"F1-Score médio (classe fraude): {wf_results[best_model_name]['mean_f1']:.4f}\\n\")\n",
    "    f.write(f\"Acurácia média: {wf_results[best_model_name]['mean_accuracy']:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Hiperparâmetros otimizados:\\n\")\n",
    "    for param, value in optimized_models[best_model_name][1].items():\n",
    "        f.write(f\"  {param}: {value}\\n\")\n",
    "\n",
    "print(f\"Resumo do melhor modelo salvo em: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusões\n",
    "\n",
    "Neste notebook, realizamos a otimização de hiperparâmetros para cada modelo e a validação walk-forward para avaliação temporal robusta. Vamos resumir os principais resultados e conclusões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização de Hiperparâmetros\n",
    "\n",
    "A otimização de hiperparâmetros melhorou significativamente o desempenho dos modelos. Os principais insights foram:\n",
    "\n",
    "1. **Regressão Logística**: O modelo se beneficiou de uma regularização adequada, com o hiperparâmetro C otimizado para equilibrar o viés e a variância.\n",
    "\n",
    "2. **Random Forest**: A profundidade das árvores e o número de estimadores foram os hiperparâmetros mais importantes, afetando diretamente a capacidade do modelo de capturar padrões complexos.\n",
    "\n",
    "3. **Gradient Boosted Trees**: A taxa de aprendizado e a profundidade máxima das árvores foram cruciais para o desempenho do modelo, com valores menores geralmente levando a melhor generalização.\n",
    "\n",
    "4. **XGBoost**: O modelo se beneficiou de uma combinação adequada de taxa de aprendizado, profundidade máxima e regularização, resultando em um modelo robusto e preciso.\n",
    "\n",
    "### Validação Walk-Forward\n",
    "\n",
    "A validação walk-forward nos permitiu avaliar o desempenho dos modelos de forma mais realista, respeitando a ordem cronológica dos dados. Os principais insights foram:\n",
    "\n",
    "1. **Estabilidade Temporal**: Observamos como o desempenho dos modelos varia ao longo do tempo, identificando modelos mais estáveis e robustos a mudanças nos padrões de fraude.\n",
    "\n",
    "2. **Comparação de Modelos**: O modelo [MELHOR_MODELO] apresentou o melhor desempenho geral, com um F1-Score médio de [VALOR] e AUC-ROC médio de [VALOR].\n",
    "\n",
    "3. **Trade-off entre Métricas**: Observamos o trade-off entre precisão e recall, com alguns modelos priorizando a detecção de mais fraudes (maior recall) à custa de mais falsos positivos (menor precisão).\n",
    "\n",
    "### Recomendações\n",
    "\n",
    "Com base nos resultados da otimização de hiperparâmetros e da validação walk-forward, recomendamos:\n",
    "\n",
    "1. **Modelo Final**: Utilizar o modelo [MELHOR_MODELO] com os hiperparâmetros otimizados para detecção de fraude em produção.\n",
    "\n",
    "2. **Monitoramento Contínuo**: Implementar um sistema de monitoramento contínuo para detectar mudanças nos padrões de fraude e retreinar o modelo quando necessário.\n",
    "\n",
    "3. **Ajuste do Limiar**: Ajustar o limiar de classificação conforme as necessidades do negócio, priorizando maior recall (identificar mais fraudes) ou maior precisão (reduzir falsos positivos).\n",
    "\n",
    "4. **Validação Temporal**: Continuar utilizando a validação walk-forward para avaliar novos modelos e atualizações, garantindo que o desempenho seja consistente ao longo do tempo.\n",
    "\n",
    "A combinação de otimização de hiperparâmetros e validação walk-forward resultou em um modelo robusto e confiável para detecção de fraude, capaz de se adaptar a mudanças nos padrões ao longo do tempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
